{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rossmann_Sales_Prediction_Capstone_Project--(ML-Regression).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushmitamuz002/Rossmann_Sales_Prediction_Capstone_Project-ML_Regression-/blob/main/Rossmann_Sales_Prediction_Capstone_Project_(ML_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOGC-qoyhJeX"
      },
      "source": [
        "# <b><u> Project Title : Sales Prediction : Predicting sales of a major store chain Rossmann</u></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y06xIdG26kRF"
      },
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        "### Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
        "\n",
        "### You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWeU1f9bwqQq"
      },
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### <b>Rossmann Stores Data.csv </b> - historical data including Sales\n",
        "### <b>store.csv </b> - supplemental information about the stores\n",
        "\n",
        "\n",
        "### <b><u>Data fields</u></b>\n",
        "### Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "* #### Id - an Id that represents a (Store, Date) duple within the test set\n",
        "* #### Store - a unique Id for each store\n",
        "* #### Sales - the turnover for any given day (this is what you are predicting)\n",
        "* #### Customers - the number of customers on a given day\n",
        "* #### Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* #### StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* #### SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* #### StoreType - differentiates between 4 different store models: a, b, c, d\n",
        "* #### Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
        "* #### CompetitionDistance - distance in meters to the nearest competitor store\n",
        "* #### CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* #### Promo - indicates whether a store is running a promo on that day\n",
        "* #### Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* #### Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* #### PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i2LlM21enWe",
        "outputId": "bf149f4b-83ff-4b80-da86-badb6a3d38da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting inflection\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: inflection\n",
            "Successfully installed inflection-0.5.1\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from Boruta) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from Boruta) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.1.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "# installing required library\n",
        "%pip install inflection\n",
        "%pip install Boruta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Libraries**"
      ],
      "metadata": {
        "id": "RslduOm_b0C2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF8l8QdCd55b"
      },
      "outputs": [],
      "source": [
        "# importing libary that are required in the porject\n",
        "import math\n",
        "import json\n",
        "import pylab\n",
        "import pickle\n",
        "import random\n",
        "import requests\n",
        "import datetime\n",
        "import warnings\n",
        "import inflection # used when renaming columns in subsection 1.1\n",
        "\n",
        "import numpy               as np\n",
        "import pandas              as pd\n",
        "import xgboost             as xgb\n",
        "import seaborn             as sns\n",
        "import matplotlib.pyplot   as plt\n",
        "import matplotlib.gridspec as gs\n",
        "\n",
        "from scipy                 import stats as ss\n",
        "from boruta                import BorutaPy\n",
        "from tabulate              import tabulate\n",
        "\n",
        "from IPython.display       import Image\n",
        "from IPython.core.display  import HTML\n",
        "\n",
        "\n",
        "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble      import RandomForestRegressor\n",
        "from sklearn.linear_model  import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
        "\n",
        "warnings.filterwarnings( 'ignore' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LU2yEc9ePk9"
      },
      "source": [
        "###**0.1 Helper Functions**\n",
        "Here there are some functions that will be helpful in the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s00rN29WfMZk",
        "outputId": "c87cc808-c249-4b6f-ec28-b8bb8567da6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Here there are some functions that will be helpful in the project\n",
        "# time series cross valdiation fuction that will be used in step 7 to find the best ML model\n",
        "def cross_validation( x_training, kfold, model_name, model, verbose = False ):\n",
        "    # creating empty lists to store te error results\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    rmse_list = []\n",
        "   \n",
        "    # iterating over a range for the k fold\n",
        "    for k in reversed (range( 1, kfold+1 ) ):\n",
        "        if verbose:\n",
        "            print( f'\\nKFold Number: {k}' )\n",
        "        # getting the start and the end dates for validation\n",
        "        validation_start_date = x_training['date'].max() - datetime.timedelta( days = k*6*7 )\n",
        "        validation_end_date = x_training['date'].max() - datetime.timedelta( days = (k-1)*6*7 )\n",
        "        \n",
        "        #filtering dataset\n",
        "        training = x_training[x_training['date'] < validation_start_date]\n",
        "        validation = x_training[x_training['date'] >= validation_end_date]\n",
        "        \n",
        "        # creating the training and validation datasets\n",
        "        # training\n",
        "        xtraining = training.drop( ['date', 'sales'], axis = 1 )\n",
        "        ytraining = training['sales']\n",
        "        \n",
        "        # validation\n",
        "        xvalidation = validation.drop( ['date', 'sales'], axis = 1 )\n",
        "        yvalidation = validation['sales']\n",
        "        \n",
        "        # setting the model\n",
        "        m = model.fit( xtraining, ytraining) # it uses the model passed in the function\n",
        "        \n",
        "        # prediction\n",
        "        yhat = m.predict( xvalidation )\n",
        "        \n",
        "        # performance\n",
        "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ))\n",
        "        \n",
        "        # storing performance of each kfold iteration\n",
        "        mae_list.append( m_result['MAE'] )\n",
        "        mape_list.append( m_result['MAPE'] )\n",
        "        rmse_list.append( m_result['RMSE'] )\n",
        "       \n",
        "    return pd.DataFrame( {'Model Name': model_name,\n",
        "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
        "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
        "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n",
        "\n",
        "\n",
        "# function to perform a correlation coeficient with categorical variables. it'll be used in section 4: EDA.\n",
        "def cramer_v( x, y ):\n",
        "    cm = pd.crosstab( x, y ).to_numpy()\n",
        "    n = cm.sum()\n",
        "    r, k = cm.shape\n",
        "    \n",
        "    chi2 = ss.chi2_contingency( cm )[0]\n",
        "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
        "    \n",
        "    #correcting cramer's V bias\n",
        "    kcorr = k - (k-1)**2/(n-1)\n",
        "    rcorr = r - (r-1)**2/(n-1)\n",
        "    \n",
        "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
        "\n",
        "# creating the mean absolute percentage error\n",
        "def mean_percentage_error( y, y_hat ):\n",
        "    return np.mean( ( y - y_hat ) / y )\n",
        "\n",
        "# creating the mean absolute percentage error\n",
        "def mean_absolute_percentage_error( y, y_hat ):\n",
        "    return np.mean( np.abs( ( y - y_hat ) / y ) )\n",
        "\n",
        "# creating a function to calculate the model error\n",
        "def ml_error( model_name, y, y_hat ):\n",
        "    mae = mean_absolute_error( y, y_hat )\n",
        "    mape = mean_absolute_percentage_error( y, y_hat )\n",
        "    rmse = np.sqrt( mean_squared_error( y, y_hat ) )\n",
        "    \n",
        "    \n",
        "    return pd.DataFrame( { 'Model Name': model_name,\n",
        "                         'MAE': mae,\n",
        "                         'MAPE': mape,\n",
        "                         'RMSE': rmse }, index = [0])\n",
        "\n",
        "# setting some notebook display as default\n",
        "def jupyter_settings():\n",
        "    %matplotlib inline\n",
        "    %pylab inline\n",
        "    \n",
        "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
        "    pd.options.display.max_columns = None\n",
        "    pd.options.display.max_rows = None\n",
        "    pd.set_option( 'display.expand_frame_repr', False )\n",
        "    \n",
        "    sns.set()\n",
        "jupyter_settings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_JJ92M4fgbN"
      },
      "outputs": [],
      "source": [
        "# diplay format(float type)\n",
        "pd.set_option( 'display.float_format', lambda x: '%.2f' % x )\n",
        "\n",
        "# setting plot parameters as default\n",
        "plt.rcParams[ 'figure.figsize' ] = [25, 8]\n",
        "plt.rcParams[ 'font.size' ] = 24\n",
        "sns.set_style( \"white\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**0.2. Loading Data**"
      ],
      "metadata": {
        "id": "CQDUD7l2wsiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv3JJ_z2hIGI",
        "outputId": "c1fe11b4-cc0e-42e7-eb81-f9a7e91c008d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ucLR1rNjUOA"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "df_Rstore = pd.read_csv(\"/content/drive/MyDrive/Datasets/Rossmann Stores Data.csv\")\n",
        "df_store = pd.read_csv(\"/content/drive/MyDrive/Datasets/store.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qwXR8_3f70j"
      },
      "outputs": [],
      "source": [
        "# Merging both the dataset on store columnn because it present on both the dataset\n",
        "df_raw = pd.merge(df_Rstore,df_store , on = 'Store', how='left' )"
      ]
    }
  ]
}